{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from generate_acts_and_attn import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model llama-3-8b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232cb8f51c56493f9f4db9ecdbc3fb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"llama-3-8b\"\n",
    "tokenizer, model, all_layers, all_attn_layers = load_model(\n",
    "    model_name=model_name, device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice is healthy. Is Alice healthy? Yes\n",
      "Alice is healthy. Is Alice unhealthy? No\n",
      "Bob is healthy. Is Bob healthy? Yes\n",
      "Bob is healthy. Is Bob unhealthy? No\n",
      "Charlie is healthy. Is Charlie healthy? Yes\n",
      "Charlie is healthy. Is Charlie unhealthy? No\n",
      "Alice is happy. Is Alice happy? Yes\n",
      "Alice is happy. Is Alice unhappy? No\n",
      "Bob is happy. Is Bob happy? Yes\n",
      "Bob is happy. Is Bob unhappy? No\n",
      "Charlie is happy. Is Charlie happy? Yes\n",
      "Charlie is happy. Is Charlie unhappy? No\n",
      "Alice is kind. Is Alice kind? Yes\n",
      "Alice is kind. Is Alice unkind? No\n",
      "Bob is kind. Is Bob kind? Yes\n",
      "Bob is kind. Is Bob unkind? No\n",
      "Charlie is kind. Is Charlie kind? Yes\n",
      "Charlie is kind. Is Charlie unkind? No\n"
     ]
    }
   ],
   "source": [
    "words = [\"healthy\", \"happy\", \"kind\"]\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "\n",
    "for word in words:\n",
    "    for name in names:\n",
    "        seqs = [\n",
    "            f\"{name} is {word}. Is {name} {word}?\",\n",
    "            f\"{name} is {word}. Is {name} un{word}?\",\n",
    "        ]\n",
    "        for seq in seqs:\n",
    "            tokens = tokenizer(seq, return_tensors=\"pt\").to(model.device).input_ids\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    tokens, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice is not unhealthy. Is Alice healthy? No\n",
      "Alice is not healthy. Is Alice healthy? No\n",
      "Bob is not unhealthy. Is Bob healthy? No\n",
      "Bob is not healthy. Is Bob healthy? No\n",
      "Charlie is not unhealthy. Is Charlie healthy? No\n",
      "Charlie is not healthy. Is Charlie healthy? No\n",
      "Alice is not unhappy. Is Alice happy? No\n",
      "Alice is not happy. Is Alice happy? No\n",
      "Bob is not unhappy. Is Bob happy? No\n",
      "Bob is not happy. Is Bob happy? No\n",
      "Charlie is not unhappy. Is Charlie happy? No\n",
      "Charlie is not happy. Is Charlie happy? No\n",
      "Alice is not unkind. Is Alice kind? No\n",
      "Alice is not kind. Is Alice kind? No\n",
      "Bob is not unkind. Is Bob kind? No\n",
      "Bob is not kind. Is Bob kind? No\n",
      "Charlie is not unkind. Is Charlie kind? No\n",
      "Charlie is not kind. Is Charlie kind? No\n"
     ]
    }
   ],
   "source": [
    "words = [\"healthy\", \"happy\", \"kind\"]\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "\n",
    "for word in words:\n",
    "    for name in names:\n",
    "        seqs = [\n",
    "            f\"{name} is not un{word}. Is {name} {word}?\",\n",
    "            f\"{name} is not {word}. Is {name} {word}?\",\n",
    "        ]\n",
    "        for seq in seqs:\n",
    "            tokens = tokenizer(seq, return_tensors=\"pt\").to(model.device).input_ids\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    tokens, max_new_tokens=1, pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaAttention(\n",
       "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (rotary_emb): LlamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_attn_layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the attention of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model llama-2-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e263cc6ec0476b9878e6fd5b8df19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"llama-2-7b\"\n",
    "tokenizer, model, all_layers, all_attn_layers = load_model(\n",
    "    model_name=model_name, device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice is not unhealthy. Is Alice healthy?\n",
      "Alice\n",
      "Alice is not healthy. Is Alice healthy?\n",
      "The two\n",
      "Bob is not unhealthy. Is Bob healthy? Is Bob not\n",
      "Bob is not healthy. Is Bob healthy?\n",
      "Bob is\n",
      "Charlie is not unhealthy. Is Charlie healthy?\n",
      "Charlie\n",
      "Charlie is not healthy. Is Charlie healthy?\n",
      "Charlie\n",
      "Alice is not unhappy. Is Alice happy?\n",
      "Alice\n",
      "Alice is not happy. Is Alice happy? No. Alice\n",
      "Bob is not unhappy. Is Bob happy?\n",
      "Bob is\n",
      "Bob is not happy. Is Bob happy?\n",
      "I'\n",
      "Charlie is not unhappy. Is Charlie happy? Charlie is happy\n",
      "Charlie is not happy. Is Charlie happy?\n",
      "Iâ€™\n",
      "Alice is not unkind. Is Alice kind? Is Alice un\n",
      "Alice is not kind. Is Alice kind?\n",
      "Alice\n",
      "Bob is not unkind. Is Bob kind?\n",
      "Bob is\n",
      "Bob is not kind. Is Bob kind?\n",
      "I'\n",
      "Charlie is not unkind. Is Charlie kind? Is Charlie cruel\n",
      "Charlie is not kind. Is Charlie kind? Is Charlie not\n"
     ]
    }
   ],
   "source": [
    "words = [\"healthy\", \"happy\", \"kind\"]\n",
    "names = [\"Alice\", \"Bob\", \"Charlie\"]\n",
    "\n",
    "for word in words:\n",
    "    for name in names:\n",
    "        seqs = [\n",
    "            f\"{name} is not un{word}. Is {name} {word}?\",\n",
    "            f\"{name} is not {word}. Is {name} {word}?\",\n",
    "        ]\n",
    "        for seq in seqs:\n",
    "            tokens = tokenizer(seq, return_tensors=\"pt\").to(model.device).input_ids\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(tokens, max_new_tokens=3)\n",
    "            print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
